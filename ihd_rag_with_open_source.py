# -*- coding: utf-8 -*-
"""IHD- RAG with Open Source

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jWpQY30HkG_vEpmpTOwV2u8bTaknj-ke
"""

# ============================================================
# COLAB RAG PIPELINE (PYTHON 3.12 FRIENDLY)
# - Upload PDF
# - Install llama-cpp-python via CPU wheels (no build from source)
# - Download GGUF to /content/mistral.gguf
# - Build LlamaIndex VectorStoreIndex with per-page chunking
# - Run a query
# ============================================================

# 0) (Recommended) Runtime -> Restart runtime

# 1) Install deps (CPU wheels) + keep versions compatible
!pip -q install -U \
  pymupdf \
  llama-index \
  llama-index-embeddings-huggingface \
  llama-index-llms-llama-cpp \
  sentence-transformers \
  huggingface_hub \
  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu

# IMPORTANT: Do NOT pin 0.2.90. Use 0.3.x to satisfy llama-index-llms-llama-cpp.
!pip -q install -U "llama-cpp-python>=0.3.0,<0.4"

# Optional: print versions for sanity
import sys, llama_cpp
print("Python:", sys.version.split()[0])
print("llama-cpp-python:", llama_cpp.__version__)

# 2) Upload PDF
from google.colab import files
import os

uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]
print("Using PDF:", pdf_path)
print("Files in /content:", os.listdir("/content"))

# 3) Extract text (PER PAGE) and build LlamaIndex Documents
import fitz  # PyMuPDF
from llama_index.core import Document

doc = fitz.open(pdf_path)

documents = []
for i, page in enumerate(doc):
    page_text = page.get_text("text")
    if page_text and page_text.strip():
        documents.append(
            Document(
                text=page_text,
                metadata={"source": pdf_path, "page": i + 1}
            )
        )

print(f"Pages: {len(doc)} | Docs created: {len(documents)}")
if documents:
    print("\n--- TEXT PREVIEW (first 800 chars of first text page) ---\n")
    print(documents[0].text[:800])

# 4) Download GGUF model to /content/mistral.gguf
from huggingface_hub import hf_hub_download

repo_id = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
filename = "mistral-7b-instruct-v0.2.Q4_K_M.gguf"

downloaded_path = hf_hub_download(
    repo_id=repo_id,
    filename=filename,
    local_dir="/content",
)

MODEL_PATH = "/content/mistral.gguf"
if downloaded_path != MODEL_PATH:
    os.replace(downloaded_path, MODEL_PATH)

print("\nModel ready at:", MODEL_PATH)
print("Files in /content now:", os.listdir("/content"))

# 5) Build RAG index + query
from llama_index.core import VectorStoreIndex
from llama_index.core.settings import Settings
from llama_index.core.node_parser import SentenceSplitter

from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# ---- Configure LLM (CPU defaults) ----
llm = LlamaCPP(
    model_path=MODEL_PATH,
    temperature=0.2,
    max_new_tokens=256,
    context_window=4096,  # increase if RAM allows; model supports larger
    model_kwargs={
        "n_gpu_layers": 0,
        "n_threads": 4,
    },
)

# ---- Embeddings ----
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

Settings.llm = llm
Settings.embed_model = embed_model

# ---- Chunking (critical for good retrieval) ----
splitter = SentenceSplitter(chunk_size=800, chunk_overlap=120)

# Build index
index = VectorStoreIndex.from_documents(documents, transformations=[splitter])

# Query
query_engine = index.as_query_engine(
    similarity_top_k=4,
    response_mode="compact"  # better for QA than tree_summarize in most cases
)

query = "What are the late payment penalties in this contract?"
response = query_engine.query(query)

print("\n====================")
print("Q:", query)
print("A:", response)
print("====================")